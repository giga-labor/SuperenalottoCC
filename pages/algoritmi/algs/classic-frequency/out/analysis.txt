Contesto e definizioni
Ho analizzato 4.141 righe (“concorsi”), ciascuna con 6 numeri (una sestina). Totale scelte = 4.141 × 6 = 24.846 numeri giocati.
Qui considero SOLO le frequenze con cui i numeri vengono scelti dall’algoritmo (non la bontà predittiva, non i “colpiti”).

Calcoli principali sulle frequenze (output osservato)
1) Copertura dello spazio dei numeri (diversificazione)
- Numeri distinti usati dall’algoritmo: 48 su 90.
- Copertura = 48/90 = 0,533… = 53,3%.
Interpretazione: l’algoritmo restringe il dominio a ~metà dei numeri disponibili.

2) Concentrazione delle scelte (quanto “spinge” su pochi numeri)
- Totale occorrenze: N = 24.846.
- Quota dei 10 numeri più usati: 76,05% di N.
  → Occorrenze “top10” ≈ 0,7605 × 24.846 ≈ 18.895
- Quota dei 5 numeri più usati: 46,33% di N.
  → Occorrenze “top5” ≈ 0,4633 × 24.846 ≈ 11.507
Interpretazione: la maggior parte delle sestine è costruita riciclando sempre lo stesso piccolo gruppo di numeri.

3) Entropia (misura sintetica della varietà)
- Entropia osservata delle scelte: H ≈ 4,17 bit
- Massimo teorico se i 90 numeri fossero scelti in modo uniforme: H_max = log2(90) ≈ 6,49 bit
- Rapporto: H/H_max ≈ 4,17/6,49 ≈ 0,64 (64%)
Interpretazione: le scelte sono significativamente meno “uniformi” di quanto sarebbe una selezione ampia e bilanciata.

Limiti intrinseci di un algoritmo basato su frequenze
1) Frequenze passate ≠ probabilità futura
In un’estrazione correttamente casuale (uniforme e indipendente), il prossimo concorso non “premia” i numeri più frequenti né “recupera” i meno frequenti.
Quindi un algoritmo che sceglie “i più frequenti” modifica solo cosa giochi, non la probabilità che escano.

2) Overfitting al rumore
Le frequenze osservate oscillano per caso su qualunque campione finito.
Un algoritmo che rincorre variazioni di frequenza (o indicatori derivati: persistenza, squilibri, trend) rischia di “imparare” fluttuazioni casuali non ripetibili.

3) Effetto imbuto: restringere troppo riduce la copertura combinatoria
Se l’algoritmo usa 48/90 numeri e concentra ~46% delle scelte sui primi 5, di fatto esplora un sottoinsieme molto ripetitivo di sestine.
Questo non crea vantaggio matematico: spesso riduce la varietà di combinazioni coperte a parità di budget.

4) Fallacie tipiche quando si ragiona a frequenze
- “Numeri caldi”: confondere alta frequenza passata con maggiore probabilità futura.
- “Ritardatari”: credere che numeri poco frequenti “debbano uscire”.
Entrambe sono interpretazioni errate in processi indipendenti.

Conclusione (responsabile e pratica)
I calcoli sulle frequenze mostrano un algoritmo fortemente concentrato: pochi numeri dominano la maggior parte delle scelte, con entropia lontana dal massimo.
Questo tipo di approccio è utile come descrizione dello stile di selezione, ma non può trasformare la frequenza storica in un vantaggio predittivo.
L’unico aumento reale di probabilità passa dal coprire più combinazioni (costo cresce linearmente); la “frequenza” da sola non crea edge."
